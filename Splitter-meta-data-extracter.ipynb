{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zefys Splitter\n",
    "\n",
    "Aufteilen und Speichern nach Zeitungsartikeln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import date\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ordner existiert bereits.\n"
     ]
    }
   ],
   "source": [
    "# abs_filepath = # relativen oder absoluten Pfad zu den Input-Dateien (xml-Dateien) angeben\n",
    "\n",
    "# Zeitung auswählen\n",
    "abbr_newspaper = 'PC' # 'NM'\n",
    "# Artikel Counter auf den richtigen Wert setzen, wenn Datensatz Neueste Mittheilungen ergänzt wird  || Refactoring! verbessern!\n",
    "\n",
    "xml_data_folder = f'\\data_{abbr_newspaper}_xml' # Datei-Ordner entsprechend benennen bzw. erstellen\n",
    "meta_data_folder = '\\meta_data' # Datei-Ordner entsprechend benennen bzw. erstellen\n",
    "txt_data_folder = f'data_{abbr_newspaper}_txt_V3' # Datei-Ordner entsprechend benennen bzw. erstellen\n",
    "\n",
    "meta_data_file = f'200524-metadaten-amtspresse-article-text.csv'\n",
    "\n",
    "path = os.path.join(abs_filepath, txt_data_folder)\n",
    "try:\n",
    "    os.mkdir(path)\n",
    "except:\n",
    "    print('Ordner existiert bereits.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29143\n"
     ]
    }
   ],
   "source": [
    "article_counter = 1\n",
    "\n",
    "if meta_data_file not in os.listdir(abs_filepath +  meta_data_folder):\n",
    "\n",
    "# Anlegen der csv-Datei mit den Metadaten\n",
    "    with open(abs_filepath + meta_data_folder + '\\\\' + meta_data_file, 'w', \n",
    "             encoding=\"utf-8\", \n",
    "             newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile, \n",
    "                           delimiter=',', \n",
    "                           quotechar='\"', \n",
    "                           quoting=csv.QUOTE_NONNUMERIC)\n",
    "        writer.writerow(['filename', 'full_date', 'year', 'year_month', 'article_count', \\\n",
    "                        'article_counts_per_issue','article_length_chars', \\\n",
    "                        'article_length_words', 'newspaper', 'headline', 'article_text'])\n",
    "\n",
    "# Iterieren über die Dateien im xml-Dateien-Ordner:\n",
    "for root, dirs, files in os.walk(abs_filepath + xml_data_folder): # || Refactoring! Nachgucken! Verbessern!\n",
    "    pass \n",
    "    \n",
    "for xml_file in files:\n",
    "    with open(abs_filepath + xml_data_folder + '\\\\' + xml_file, 'r', encoding='utf8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    text_header, text_body = text.split('</datum>') # abtrennen des Headers\n",
    "    try:\n",
    "        text_articles, text_footer = text_body.split('<fusszeile>') # abtrennen des Footers\n",
    "        article_list = text_articles.split('<hr/>') # aufteilen der Zeitung bei <hr/>\n",
    "    except:\n",
    "        article_list = text_body.split('<hr/>')\n",
    "\n",
    "    for article in article_list:   \n",
    "     \n",
    "        if article.count('<untertitel>') >= 2: # mit <untertitel> splitten\n",
    "            article_splits = article.split('<untertitel>')\n",
    "            article_splits.remove(article_splits[0]) #  xml vor 1. <untertitel> löschen\n",
    "            article_list.extend(article_splits)       \n",
    "\n",
    "    counter = 1\n",
    "\n",
    "    for article in article_list:\n",
    "        \n",
    "        # Überschriften mit regex herausfiltern || Refactoring? Alternative Beautiful Soup?\n",
    "        pattern = re.compile('<fett[^>]*>(.+?)</fett>')\n",
    "        heading = re.search(pattern, article)         \n",
    "        \n",
    "        # print(heading)\n",
    "        if heading != None:\n",
    "            headline = heading[0]\n",
    "            headline = re.sub('<[^>]*>', \"\", headline)\n",
    "            \n",
    "        if 'Wochenschau' not in headline and heading == None:\n",
    "            headline = \"Ohne Überschrift\"\n",
    "    \n",
    "        # print(article_counter, headline)\n",
    "        \n",
    "        # xml-tag entfernen, um weitere Schritte durchzuführen\n",
    "        article = article.replace('</titelkopf>','')\n",
    "\n",
    "        # xml-tags entfernen\n",
    "        soup = BeautifulSoup(article) \n",
    "        article_without_xml = soup.get_text()     \n",
    "\n",
    "        # entfernen von nicht gewolltem Mark-Up-Text\n",
    "        article_without_xml = re.sub( \n",
    "            r'sachkommentar\\d+\\*\\d-\\d\\d-\\d', \n",
    "            '', \n",
    "            article_without_xml)\n",
    "        \n",
    "        # Datumsformat aus xml-Dateiname-konvertieren\n",
    "        date_format = pd.to_datetime(xml_file[:-4]) \n",
    "        \n",
    "        # Zuordnen der Variablen für Metadaten-Datei\n",
    "        filename = xml_file[:-4] + '_' + str(counter).zfill(2) + '.txt'              \n",
    "        full_date = xml_file[:-4]\n",
    "        year = date_format.strftime('%Y')\n",
    "        year_month = date_format.strftime('%Y-%m')\n",
    "        article_count = article_counter\n",
    "        article_counts_per_issue = len(article_list)\n",
    "        article_length_chars = len(article)\n",
    "        article_length_words = len(article.split(' '))\n",
    "        headline = headline\n",
    "        newspaper = abbr_newspaper\n",
    "        article_text = article_without_xml.strip()           \n",
    "        \n",
    "\n",
    "        # Einstellen der Artikel Länge, die nicht beachtet werden soll\n",
    "        if article_length_words < 5:  \n",
    "            continue      \n",
    "\n",
    "        # Speichern der Metadaten in einer Zeile der csv-Datei\n",
    "        \n",
    "        with open(abs_filepath + meta_data_folder + '\\\\' + meta_data_file, 'a', \n",
    "                 encoding=\"utf-8\", \n",
    "                 newline='') as csvfile:\n",
    "            writer = csv.writer(csvfile, \n",
    "                               delimiter=',', \n",
    "                               quotechar='\"', \n",
    "                               quoting=csv.QUOTE_NONNUMERIC)\n",
    "            writer.writerow([filename, full_date, year, year_month, article_count, \\\n",
    "                            article_counts_per_issue, article_length_chars, \\\n",
    "                            article_length_words, newspaper, headline, article_text])\n",
    "\n",
    "        # Speichern der einzelnen Artikel in txt-Dateien\n",
    "#         with open(path + '\\\\' + filename, 'w', encoding='utf8') as f:\n",
    "#             f.write(article_text)\n",
    "\n",
    "        # Counter hochzählen\n",
    "        counter +=1\n",
    "        article_counter +=1\n",
    "print(article_counter)"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
